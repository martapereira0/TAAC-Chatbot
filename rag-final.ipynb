{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60fee9f-e907-49a7-8387-8937a1618a87",
   "metadata": {},
   "source": [
    "'QhdyPvS28vnpVY94g5YKmm58PgEhUCYb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24f8279-278a-4533-aa21-60e770655e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32aabc-07cc-47d2-8713-334e0af6222f",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e67e77-460f-4e0c-a363-89a175ee57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "!pip install --quiet langchain-openai\n",
    "!pip install --quiet pypdf\n",
    "!pip install --quiet sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd68b3-55d3-4fdb-9b91-65fcd91c5281",
   "metadata": {},
   "source": [
    "# Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08816c10-5cec-476d-b673-83fa40c1b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser  # For the chains\n",
    "from langchain_core.runnables import RunnablePassthrough   # For the chains\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader #For PDF Loader\n",
    "from langchain_mistralai import ChatMistralAI #For MISTRAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed88fe4-471b-4af1-879f-b5990b11628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting uup langsmith:\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"taac-rag\"\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9442ca9d-511a-4a61-9555-b610d4521243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6964111-5723-4fc0-b918-3846ec8de5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"USER_AGENT\"] = \"RAG-Chatbot\"\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5651df-5764-4a12-8c37-611d1ec2f6f0",
   "metadata": {},
   "source": [
    "# Selected Model: Mistral 7B\n",
    "\n",
    "We have selected the **Mistral model** with **7 billion parameters**, and we are accessing it remotely using a Mistral API Key.\n",
    "\n",
    "## Reasons for Selection\n",
    "1. **Open-Source**: Being open-source allows for customization and adaptability.\n",
    "2. **Relatively Small Size**: With 7B parameters, it offers a good balance between performance and computational cost, making it feasible to use locally with proper quantization.\n",
    "3. **Proven Performance**: The model has shown excellent results across various tasks, proving effective in multiple contexts.\n",
    "\n",
    "*Mistral context window has 32.8k size which is proximatly 20,000–25,000 words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74691e4f-7c9c-4697-9c92-892794e8e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"open-mistral-7b\",api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33088e-9c88-45c9-8d01-b934000f4085",
   "metadata": {},
   "source": [
    "# 1-RAG PIPELINE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcadfb8-60d9-465b-b589-1a29c90cb6b8",
   "metadata": {},
   "source": [
    "## Indexing: Load (PDF)\n",
    "We use PyPDFLoader for loading local pdf, but we mihgt change that for web Documents loadin, later with  **WebBaseLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce9dc07-b667-49ad-811c-c361d8af32ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1882"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "# Replace this with the path to your local PDF file\n",
    "pdf_file_path = \"ArtificialIntelligenceAct-1-50.pdf\"\n",
    "# Load the local PDF file\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "# Load and process the document\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs[0].page_content) #here docs is already an LangChain Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf307ce4-23db-484d-b376-5ddd8bcc5699",
   "metadata": {},
   "source": [
    "## Indexing: Split\n",
    "\n",
    "we use Chroma as our Vector Store\n",
    "we use all-MiniLM-L6-v2 from Microsoftmodel to create the Embeddings. (OpenAI are pay to use)\n",
    "\n",
    "The chunk size is an balanced Value considering on the Mistral and embedding model context window size which seems to work good on practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b786a0bd-ed5e-488d-8097-b73cae4b3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ac69c-56d4-4a81-b4b3-3a7349cb37a6",
   "metadata": {},
   "source": [
    "# Indexing: Store Embeddings\n",
    "- **Chroma** is an open-source **vector database** that’s designed for scalable, high-performance **similarity search**.\n",
    "- The model **all-MiniLM-L6-v2** is part of the MiniLM (Mini Language Models) family developed by **Microsoft**. Especially suited for **semantic similarity** tasks, **sentence embedding**, and **question-answer retrieval**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f38d84-1a6d-4f19-896e-536d34f71306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/vmqjv96x06n7r3mgqqg07fsw0000gn/T/ipykernel_64692/1528764191.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  vectorstore = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
      "/Users/gmonteiro/miniconda3/envs/taac/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d22a39-8e4d-483c-864b-620c022695a3",
   "metadata": {},
   "source": [
    "##  Retrieval and Generation: Retrieve\n",
    "## Retreiver:\n",
    "We're using the most common type of Retriever wich is the VectorStoreRetriever.\n",
    "## Prompt:\n",
    "The prompt that is being pulled from *https://smith.langchain.com/hub/rlm/rag-prompt*: \n",
    "\n",
    "\"\n",
    "`HUMAN`\n",
    "\n",
    "`You are an assistant for question-answering tasks. Use the following retrieved context to answer the question. If you don't know the answer, state that clearly. Limit your response to three sentences, keeping the answer concise.`\n",
    "\n",
    "\n",
    "`Question: {question}`\n",
    "\n",
    "`Context: {context}`\n",
    "\n",
    "`Answer:`\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70eb0494-ea60-4002-9bf9-4ace03a83f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a143ba-dcf9-42f5-8cd4-4a430d0e2f09",
   "metadata": {},
   "source": [
    "## Retrieval and Generation: Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3979ec7-a9e4-4e78-84cb-275735fa3b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The AI Act, according to Union law, aims to improve the internal market by laying down a uniform framework for AI systems. It promotes the uptake of human-centric and trustworthy AI while ensuring a high level of protection of fundamental rights, including democracy, the rule of law, and environmental protection. The Act also fosters the development and use of AI, ensuring free movement of AI-based goods and services, but only under harmonized rules to protect public interests and fundamental rights.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()  #becaouse some meta_data might come along with the text. Only extract the text\n",
    ")\n",
    "rag_chain.invoke(\"What are the main values enshrined in the AI Act, according to Union law?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb57686-1cf9-49d4-9791-ed106a78f5ee",
   "metadata": {},
   "source": [
    "# 2-RAG EVALUTAION\n",
    "\n",
    "## RAG Evaluation Types\n",
    "\n",
    "In Retrieval-Augmented Generation (RAG) systems, there are four primary evaluation types that users are commonly interested in:\n",
    "\n",
    "### 1. Response vs. Reference Answer\n",
    "- **Goal**: Assess the accuracy of the RAG chain response in relation to a known, ground-truth answer.\n",
    "- **Evaluation Mode**: Requires a reference answer, typically provided via a dataset, to compare against.\n",
    "- **Judge**: Uses a language model as the evaluator to determine answer correctness by comparing the generated response to the ground-truth answer.\n",
    "\n",
    "### 2. Response vs. Input Question\n",
    "- **Goal**: Evaluate how effectively the generated response addresses the user's initial question or input.\n",
    "- **Evaluation Mode**: Does not require a reference answer; instead, it compares the answer directly with the input question.\n",
    "- **Judge**: Uses a language model as the evaluator to assess relevance, helpfulness, and whether the response meets the user's intent.\n",
    "\n",
    "### 3. Response vs. Retrieved Documents\n",
    "- **Goal**: Determine the consistency between the generated response and the retrieved documents, focusing on factual accuracy and faithfulness.\n",
    "- **Evaluation Mode**: Does not require a reference answer; compares the answer to the context retrieved during the RAG process.\n",
    "- **Judge**: Uses a language model as the evaluator to check for faithfulness, detect hallucinations, and ensure alignment with the provided context.\n",
    "\n",
    "> **Note**: We will not be using this evaluation type in this project, as it is more useful for evaluating the model itself and assessing the prompt used for RAG.\n",
    "\n",
    "### 4. Retrieved Documents vs. Input Question\n",
    "- **Goal**: Measure the relevance and quality of the retrieved documents with respect to the user’s original query.\n",
    "- **Evaluation Mode**: Reference-free; evaluates the retrieved documents based on their relevance to the input question.\n",
    "- **Judge**: Uses a language model as the evaluator to judge relevance, ensuring that the retrieved information is pertinent to the query.\n",
    "\n",
    "---\n",
    "\n",
    "##### In this project, we will concentrate on evaluation types **1** and **2**, as they are the most pertinent to our objectives. We’ve chosen to exclude evaluation type **3** since it is primarily useful for assessing the model's performance and the effectiveness of the prompt within the RAG process. Furthermore, we are not also implementing evaluation type **4**, as the behavior of the retriever can be easily monitored through LangSmith tracing, making a separate evaluation unnecessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f003c87-6739-48fd-8b95-6e609e8777e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "import httpx\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a7961-7a5a-416e-b722-acd35718da97",
   "metadata": {},
   "source": [
    "## The Data-set used for Evaluation\n",
    "\n",
    "#### Dataset with some LCEL(LangChain Expression Language) questions as input and and exepected output/answear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764dcf32-c826-43d5-a5f4-4d9d6de93695",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.clone_public_dataset(\"https://smith.langchain.com/public/730d833b-74da-43e2-a614-4e2ca2502606/d\")\n",
    "dataset_name = \"LCEL-QA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1b4f3-d229-40cf-8057-56c32b258793",
   "metadata": {},
   "source": [
    "# Function for RAG invocation\n",
    "We are implementing time delays and staggered execution to prevent API calls to Mistral from being throttled due to rate limits. By incorporating time.sleep and exponential backoff, we aim to manage the timing of requests, reducing the likelihood of hitting the rate limit and ensuring smoother interaction with the Mistral API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff4f9e2-679f-4023-900f-e9e05bebddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_rag_answer(example: dict):\n",
    "    # Applying exponential backoff if necessary here as well\n",
    "    max_retries = 7\n",
    "    delay = 7  # Start with a 5-second delay\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return rag_chain.invoke(example[\"input_question\"])\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429 and attempt < max_retries - 1:\n",
    "                print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                return \"Error: Rate limit exceeded\"\n",
    "\n",
    "def generate_rag_answer_with_context(example: dict):\n",
    "    #Use this for evaluation of retrieved documents and hallucinations with exponential backoff for rate limiting.\n",
    "    max_retries = 7\n",
    "    delay = 7  # Start with a 10-second delay\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Make the request to get the answer and contexts\n",
    "            response = rag_chain.invoke(example[\"input_question\"])\n",
    "            return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429 and attempt < max_retries - 1:\n",
    "                print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                return {\"answer\": \"Error: Rate limit exceeded\", \"contexts\": []}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071a34d-bcf1-4593-aa91-4121dcfde3d0",
   "metadata": {},
   "source": [
    "# 1 eval : Response vs reference answer\n",
    "Here, we are comparing the response generated by the RAG invocation with the ground truth or desired response provided in the dataset. The approach involves specifying a prompt that instructs the model to answer a question. After generating its response, the model then compares its answer with the labeled (ground truth) response, assigning a score of 1 if the answer is close to the desired response, and 0 otherwise. Finally, we calculate the average score across 20 examples to assess overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a9e4f9a-7f00-44fc-b320-02cc3c93931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy with exponential backoff for rate limiting.\n",
    "    \"\"\"\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs\n",
    "\n",
    "    llm = ChatMistralAI(model=\"open-mistral-7b\",api_key=mistral_api_key)\n",
    "\n",
    "    # Structured grading prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Set up exponential backoff parameters\n",
    "    max_retries = 7\n",
    "    delay = 7  # Start with a 5-second delay\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            score = answer_grader.invoke({\n",
    "                \"question\": input_question,\n",
    "                \"correct_answer\": reference,\n",
    "                \"student_answer\": prediction\n",
    "            })\n",
    "            return {\"key\": \"answer_v_reference_score\", \"score\": score[\"Score\"]}\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429 and attempt < max_retries - 1:\n",
    "                print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 3  # Exponential backoff\n",
    "            else:\n",
    "                return {\"key\": \"answer_v_reference_score\", \"score\": \"Error: Rate limit exceeded\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5c2c7ae-0a59-4f50-a3c5-eb6871a6360b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-v-reference-a89baf6b' at:\n",
      "https://smith.langchain.com/o/2556513b-85f5-4f8a-a6cc-ef8f17fb2ac6/datasets/c421821d-9bf4-4706-904b-359aad37fb34/compare?selectedSessions=cbba98a0-d6ae-41e5-bfe2-0f4bc9e5f03e\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3dd3c372ac54f4f89d5ccb2c2e681b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 63 seconds...\n",
      "Rate limit exceeded. Retrying in 63 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 80 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 20 seconds...\n",
      "Rate limit exceeded. Retrying in 189 seconds...\n",
      "Rate limit exceeded. Retrying in 63 seconds...\n",
      "Rate limit exceeded. Retrying in 63 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 40 seconds...\n",
      "Rate limit exceeded. Retrying in 160 seconds...\n",
      "Rate limit exceeded. Retrying in 160 seconds...\n",
      "Rate limit exceeded. Retrying in 160 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 160 seconds...\n",
      "Rate limit exceeded. Retrying in 160 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 189 seconds...\n",
      "Rate limit exceeded. Retrying in 189 seconds...\n",
      "Rate limit exceeded. Retrying in 63 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 21 seconds...\n",
      "Rate limit exceeded. Retrying in 567 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation 20 exemples\n",
    "experiment_results = evaluate(\n",
    "    generate_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-answer-v-reference\",\n",
    "    metadata={\"version\": \"LCEL context, mistral7B\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586d40d-794d-4cbb-8411-f56f842555fc",
   "metadata": {},
   "source": [
    "# 2 eval: Response vs input\n",
    "\n",
    "The same approach is used for the Response vs. Input evaluation. Here, the model is responsible for generating answers and then evaluating its own responses based on the criteria provided in the prompt.\n",
    "\n",
    "In this case, the prompt instructs the model to assign a score of 1 (good) or 0 (bad) according to these guidelines:\n",
    "1. The STUDENT ANSWER should be concise and directly relevant to the QUESTION.\n",
    "2. The STUDENT ANSWER should contribute toward answering the QUESTION effectively. \n",
    "\n",
    "This process allows the model to assess its responses objectively, ensuring they are both relevant and helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4460820-ff32-455d-a89f-2b68becec874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_helpfulness = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer helpfulness with exponential backoff for rate limiting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question and RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    prediction = run.outputs\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatMistralAI(model=\"open-mistral-7b\", api_key=mistral_api_key, temperature=0)\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Set up exponential backoff parameters\n",
    "    max_retries = 10\n",
    "    delay = 10  # Start with a 10-second delay\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            score = answer_grader.invoke({\n",
    "                \"question\": input_question,\n",
    "                \"student_answer\": prediction\n",
    "            })\n",
    "            return {\"key\": \"answer_helpfulness_score\", \"score\": score[\"Score\"]}\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429 and attempt < max_retries - 1:\n",
    "                print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 3  # Exponential backoff\n",
    "            else:\n",
    "                return {\"key\": \"answer_helpfulness_score\", \"score\": \"Error: Rate limit exceeded\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f480ce4b-70e3-4a56-92e6-6d162d448932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-helpfulness-967a4e62' at:\n",
      "https://smith.langchain.com/o/2556513b-85f5-4f8a-a6cc-ef8f17fb2ac6/datasets/c421821d-9bf4-4706-904b-359aad37fb34/compare?selectedSessions=a0563944-e838-436c-9ae3-e9a988fb81e9\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efeed63f7c69434ab416e97c960b9472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 90 seconds...\n",
      "Rate limit exceeded. Retrying in 90 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 7 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 56 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 14 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 28 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 90 seconds...\n",
      "Rate limit exceeded. Retrying in 90 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 112 seconds...\n",
      "Rate limit exceeded. Retrying in 112 seconds...\n",
      "Rate limit exceeded. Retrying in 112 seconds...\n",
      "Rate limit exceeded. Retrying in 112 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n",
      "Rate limit exceeded. Retrying in 112 seconds...\n",
      "Rate limit exceeded. Retrying in 112 seconds...\n",
      "Rate limit exceeded. Retrying in 30 seconds...\n",
      "Rate limit exceeded. Retrying in 224 seconds...\n",
      "Rate limit exceeded. Retrying in 224 seconds...\n",
      "Rate limit exceeded. Retrying in 224 seconds...\n",
      "Rate limit exceeded. Retrying in 10 seconds...\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    generate_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_helpfulness_evaluator],\n",
    "    experiment_prefix=\"rag-answer-helpfulness\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122927f9-ca1a-4cb1-83b3-36cad4af4413",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "As shown in the results, we achieved:\n",
    "- A score of 0.85 for answer helpfulness\n",
    "- A score of 0.60 for response vs. reference answer accuracy\n",
    "  \n",
    "Considering that the evaluation dataset was limited in size and some instances were affected by rate limits on Mistral API calls, we can conclude that the benchmark positively reflects the effectiveness of the RAG pipeline we developed. These scores suggest that the pipeline performs well in terms of relevance and helpfulness, despite the constraints encountered.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
